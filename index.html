<!DOCTYPE html>
<html lang="en">
  <head>
    <title>WorldModelBench</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script
      src="https://kit.fontawesome.com/f8ddf9854a.js"
      crossorigin="anonymous"
    ></script>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="A Multi-discipline and Multi-domain Benchmark
      for Expert World Models"
    />
    <meta
      name="keywords"
      content="WorldModelBench, World Model, World Model Evaluation, Vision Language Model, Large Language Model, Large Multimodal Model, artificial intelligence, AI, AGI, artificial general intelligence"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      WorldModelBench: Judging Video Generation Models As World Models
    </title>

    <link rel="icon" href="./static/images/wmb/worldmodelbench_icon.png" />

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/index.css" />
    <script
      src="https://kit.fontawesome.com/fff5b27ec1.js"
      crossorigin="anonymous"
    ></script>
    <!-- <script src="https://kit.fontawesome.com/eaf1856e6f.js" crossorigin="anonymous"></script> -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a
          role="button"
          class="navbar-burger"
          aria-label="menu"
          aria-expanded="false"
        >
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center">
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link"> More Research </a>
            <div class="navbar-dropdown">
              <a
                class="navbar-item"
                href="https://huggingface.co/datasets/MMMU/MMMU_Pro"
              >
                <b>MMMU-Pro</b>
                <span style="font-size: 18px; display: inline; margin-left: 5px"
                  >ðŸ”¥</span
                >
              </a>
            </div>
          </div>
        </div>
      </div>
    </nav>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title is-bold">
                <img
                  src="static/images/wmb/worldmodelbench_icon.png"
                  style="width: 1em; vertical-align: middle"
                  alt="Logo"
                />
                <span class="wmb" style="vertical-align: middle"
                  >WorldModelBench</span
                >
              </h1>
              <h2 class="subtitle is-3 publication-subtitle">
                A Multi-discipline and Multi-domain Benchmark for Expert World
                Models
              </h2>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a
                    href="https://dachengli1.github.io/"
                    style="text-decoration: none; color: inherit"
                    >Dacheng Li*â€ </a
                  >,
                </span>
                <span class="author-block">
                  <a
                    href="https://seerkfang.github.io/"
                    style="text-decoration: none; color: inherit"
                    >Yunhao Fang*â€ </a
                  >,
                </span>
                <br />
                <span class="author-block">Yukang Chen,</span>
                <span class="author-block">Shuo Yang,</span>
                <span class="author-block">Shiyi Cao,</span>
                <span class="author-block">Justin Wong,</span><br />
                <span class="author-block">Xiaolong Wang,</span>
                <span class="author-block">Hongxu Yin,</span>
                <span class="author-block">Joseph E. Gonzalez,</span>
                <span class="author-block">Ion Stoica,</span><br />
                <span class="author-block">
                  <a
                    href="https://hanlab.mit.edu/songhan/"
                    style="text-decoration: none; color: inherit"
                    >Song Han*</a
                  >
                </span>
                <span class="author-block">
                  <a
                    href="https://research.nvidia.com/person/yao-lu-jason"
                    style="text-decoration: none; color: inherit"
                    >Jason Lu*â€ </a
                  >
                </span>
              </div>

              <br />

              <div class="is-size-5 publication-authors">
                <span class="author-block"><b>WorldModelBench Team</b></span>
              </div>

              <br />
              <div class="is-size-5 publication-authors">
                <span class="author-block">*Core Contributors</span><br />
                <span class="author-block">â€ Corresponding to:</span>
                <span class="author-block"
                  ><a href="mailto:dacheng177@berkeley.edu"
                    >dacheng177@berkeley.edu</a
                  >,</span
                >
                <span class="author-block"
                  ><a href="mailto:seerkfang@gmail.com">seerkfang@gmail.com</a
                  >,</span
                >
                <span class="author-block"
                  ><a href="mailto:">jasonlu@nvidia.com</a></span
                >
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/2311.16502"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://huggingface.co/datasets/MMMU/MMMU"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon" style="font-size: 18px">ðŸ¤—</span>
                      <span>WorldModelBench</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://github.com/MMMU-Benchmark/MMMU"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="#leaderboard"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-trophy"></i>
                      </span>
                      <span>Leaderboard</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://eval.ai/web/challenges/challenge-page/2179/overview"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-medal"></i>
                      </span>
                      <span>EvalAI</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://twitter.com/xiangyue96/status/1729698316554801358"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon has-text-white">
                        <i class="fa-brands fa-x-twitter"></i>
                      </span>
                      <span>Twitter</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop has-text-centered">
        <img src="static/images/wmb/overview.jpg" alt="geometric reasoning" />
        <p>
          Overview of our <b>WorldModelBench</b>. WorldModelBench introduces
          three key features: 1) <b>Comprehensive</b> Domains: It spans 7
          domains and 56 subdomains, offering a total of 350 images and text
          conditions tailered for video generation; 2) <b>Nuanced</b> World
          Modeling Criteria: Instruction Following, Common Sense, and
          <b>Physical Adherence</b>; 3) <b>Human-Aligned</b> Evaluation:
          WorldModelBench incorporates a human-aligned judge to assess world
          modeling performance.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container" style="margin-bottom: 2vh">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">ðŸ””News</h2>
            <div class="content has-text-justified">
              <p>
                <b
                  >ðŸ”¥[2024-12-05] Introducing
                  <a href="https://arxiv.org/abs/2409.02813">WorldModelBench</a
                  >, a comprehensive benchmark for world model capacities'
                  evaluation! ðŸš€</b
                >
              </p>
            </div>
            <h2 class="title is-3">Introduction</h2>
            <div class="content has-text-justified">
              <p>
                We introduce WorldModelBench, a benchmark designed to evaluate
                the <b>world modeling capabilities</b> of video generation
                models across 7 application-driven domains and 56 subdomains,
                offering <b>two key advantages</b>: (1)
                <b>Against to nuanced world modeling violations</b>: By
                incorporating instruction-following and physics-adherence
                dimensions, WorldModelBench detects subtle violations, such as
                irregular changes in object size that breach the mass
                conservation lawâ€”issues overlooked by prior benchmarks. (2)
                <b>Aligned with large-scale human preferences</b>: We
                crowd-source <b>67K</b> human labels to accurately measure
                <b>14</b> frontier models. Using our high-quality human labels,
                we further fine-tune an accurate judger to automate the
                evaluation procedure, achieving <b>8.6%</b> higher avereage
                accuracy in predicting world modeling violations than GPT-4o
                with 2B parameters. We believe WorldModelBench will stimulate
                the community to build next-generation world models towards
                expert world simulators.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
      </div>
    </section>

    <!-- DATASET SECTION -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">
          <img
            src="static/images/wmb/worldmodelbench_icon.png"
            alt="Logo"
            class="mmmu-logo"
          />
          <span class="mmmu">WorldModelBench</span>
        </h1>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Overview</h2>
            <div class="content has-text-justified">
              <p>
                We introduce WorldModelBench, a <b>multi-discipline</b> and
                <b>multi-domain</b> benchmark Understanding and Reasoning (MMMU)
                benchmark, a novel benchmark meticulously curated to assess the
                expert-level multimodal understanding capability of foundation
                models across a broad scope of tasks. Covering subjects across
                disciplines, including Art, Business, Health & Medicine,
                Science, Humanities & Social Science, and Tech & Engineering,
                and over subfields. The detailed subject coverage and statistics
                are detailed in the figure. The questions in our benchmark were
                manually collected by a team of college students (including
                coauthors) from various disciplines and subjects, drawing from
                online sources, textbooks, and lecture materials.
              </p>
              <img
                src="static/images/mmlu_example.Jpeg"
                alt="algebraic reasoning"
                class="center"
              />
              <br />
              <p>
                MMMU is designed to measure three essential skills in LMMs:
                perception, knowledge, and reasoning. Our aim is to evaluate how
                well these models can not only perceive and understand
                information across different modalities but also apply reasoning
                with subject-specific knowledge to derive the solution.
              </p>
              <p>
                Our MMMU benchmark introduces key challenges to multimodal
                foundation models, as detailed in a figure. Among these, we
                particularly highlight the challenge stemming from the
                requirement for both expert-level visual perceptual abilities
                and deliberate reasoning with subject-specific knowledge. This
                challenge is vividly illustrated through our tasks, which not
                only demand the processing of various heterogeneous image types
                but also necessitate a model's adeptness in using
                domain-specific knowledge to deeply understand both the text and
                images and to reason. This goes significantly beyond basic
                visual perception, calling for an advanced approach that
                integrates advanced multimodal analysis with domain-specific
                knowledge.
              </p>
            </div>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
            <div class="content has-text-justified">
              <p>
                To further distinguish the difference between <i>dataset</i> and
                other existing ones, we elaborate the benchmark details in
                Figure. From the <i>breadth</i> perspective, the prior
                benchmarks are heavily focused on daily knowledge and common
                sense. The covered image format is also limited. Our benchmark
                aims to cover college-level knowledge with 30 image formats
                including diagrams, tables, charts, chemical structures, photos,
                paintings, geometric shapes, music sheets, medical images, etc.
                In the <i>depth</i> aspect, the previous benchmarks normally
                require commonsense knowledge or simple physical or temporal
                reasoning. In contrast, our benchmark requires deliberate
                reasoning with college-level subject knowledge.
              </p>
              <div class="content has-text-centered">
                <img
                  src="static/images/compare.Jpeg"
                  alt="algebraic reasoning"
                  class="center"
                />
                <p>
                  Sampled MMMU examples from each discipline. The questions and
                  images need expert-level knowledge to understand and reason.
                </p>
              </div>
            </div>
          </div>
        </div>

        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3">Statistics</h2>
            <div class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img
                    src="static/images/mmmu_subject_distribution.Jpeg"
                    alt="algebraic reasoning"
                    width="95%"
                  />
                  <p>
                    Sampled MMMU examples from each discipline. The questions
                    and images need expert-level knowledge to understand and
                    reason.
                  </p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img
                    src="static/images/statistics.png"
                    alt="arithmetic reasoning"
                    width="40%"
                  />
                  <p>Key statistics of the MMMU benchmark</p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img
                    src="static/images/image_type_count.png"
                    alt="arithmetic reasoning"
                    width="80%"
                  />
                  <p>Distribution of image types in the MMMU dataset</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- RESULTS SECTION -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">Experiment Results</h1>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
            <div class="content has-text-justified">
              <p>
                We evaluate various models including LLMs and LMMs. In each
                type, we consider both closed- and open-source models. Our
                evaluation is conducted under a zero-shot setting to assess the
                capability of models to generate accurate answers without
                fine-tuning or few-shot demonstrations on our benchmark. For all
                models, we use the default prompt provided by each model for
                multi-choice or open QA, if available. If models do not provide
                prompts for task types in MMMU, we conduct prompt engineering on
                the validation set and use the most effective prompt for the
                later zero-shot experiment.
              </p>
            </div>
            <br />
            <div class="model-labels-container">
              <span class="leaderboard-label human_expert">Human Expert</span>
              <span class="leaderboard-label open_source">Open-Source</span>
              <span class="leaderboard-label proprietary">Proprietary</span>
            </div>
            <br />
            <div class="content has-text-centered">
              <p>
                Click on MMMU-Pro, MMMU (Val) or MMMU (Test) to expand detailed
                results.
              </p>
            </div>
            <div class="leaderboard-container">
              <div class="table-wrapper">
                <table id="mmmu-table">
                  <thead>
                    <tr>
                      <th
                        colspan="3"
                        class="reset-cell clickable"
                        style="text-align: center"
                      >
                        Reset
                      </th>
                      <th class="pro-details-cell clickable" colspan="1">
                        MMMU-Pro
                      </th>
                      <th class="val-details-cell clickable" colspan="1">
                        MMMU(Val)
                      </th>
                      <th class="test-details-cell clickable" colspan="1">
                        MMMU(Test)
                      </th>
                    </tr>
                    <tr>
                      <th class="sortable clickable" data-sort="string">
                        Name
                      </th>
                      <th class="clickable" data-sort="string">Size</th>
                      <th class="sortable clickable" data-sort="date">Date</th>
                      <th
                        class="sortable clickable pro-overall"
                        data-sort="number"
                      >
                        Overall
                      </th>
                      <th
                        class="hidden pro-details sortable clickable"
                        data-sort="number"
                      >
                        Vision
                      </th>
                      <th
                        class="hidden pro-details sortable clickable"
                        data-sort="number"
                      >
                        Standard
                      </th>
                      <th
                        class="sortable clickable val-overall"
                        data-sort="number"
                      >
                        Overall
                      </th>
                      <th
                        class="hidden val-details sortable clickable"
                        data-sort="number"
                      >
                        Art & Design
                      </th>
                      <th
                        class="hidden val-details sortable clickable"
                        data-sort="number"
                      >
                        Business
                      </th>
                      <th
                        class="hidden val-details sortable clickable"
                        data-sort="number"
                      >
                        Science
                      </th>
                      <th
                        class="hidden val-details sortable clickable"
                        data-sort="number"
                      >
                        Health & Medicine
                      </th>
                      <th
                        class="hidden val-details sortable clickable"
                        data-sort="number"
                      >
                        Human. & Social Sci.
                      </th>
                      <th
                        class="hidden val-details sortable clickable"
                        data-sort="number"
                      >
                        Tech & Eng.
                      </th>
                      <th
                        class="sortable clickable test-overall"
                        data-sort="number"
                      >
                        Overall
                      </th>
                      <th
                        class="hidden test-details sortable clickable"
                        data-sort="number"
                      >
                        Art & Design
                      </th>
                      <th
                        class="hidden test-details sortable clickable"
                        data-sort="number"
                      >
                        Business
                      </th>
                      <th
                        class="hidden test-details sortable clickable"
                        data-sort="number"
                      >
                        Science
                      </th>
                      <th
                        class="hidden test-details sortable clickable"
                        data-sort="number"
                      >
                        Health & Medicine
                      </th>
                      <th
                        class="hidden test-details sortable clickable"
                        data-sort="number"
                      >
                        Human. & Social Sci.
                      </th>
                      <th
                        class="hidden test-details sortable clickable"
                        data-sort="number"
                      >
                        Tech & Eng.
                      </th>
                    </tr>
                  </thead>
                  <tbody>
                    <!-- Table body will be populated dynamically -->
                  </tbody>
                </table>
                <p class="test-desc">
                  Overall results of different models on the MMMU leaderboard.
                  The best-performing model in each category is <b>in-bold</b>,
                  and the second best is <u>underlined</u>. *: results provided
                  by the authors.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- @PAN TODO: bibtex -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title is-3 has-text-centered">BibTeX</h2>
        <pre><code>
          @inproceedings{yue2023mmmu,
            title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
            author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
            booktitle={Proceedings of CVPR},
            year={2024},
          }
    </code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is website adapted from
              <a href="https://nerfies.github.io/">Nerfies</a> and
              <a href="https://mmmu-benchmark.github.io/">MMMU</a>, licensed
              under a
              <a
                rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/"
                >Creative Commons Attribution-ShareAlike 4.0 International
                License</a
              >.
            </p>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
